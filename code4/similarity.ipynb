{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup, BertConfig, BertForMaskedLM\n",
    "model_path = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "bert_lm = BertForMaskedLM.from_pretrained(model_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "inputs = [\"The Battle of Dunbar\", \n",
    "          \"Wow that is so good!\"]\n",
    "inputs = tokenizer(inputs, max_length=10, padding='max_length', return_tensors='pt', truncation=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1103, 2321, 1104, 3840, 1179, 6824,  102,    0,    0],\n",
       "        [ 101,  192, 4064, 1115, 1110, 1177, 1363,  106,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "last_hidden_state = bert_lm.bert(**inputs).last_hidden_state\n",
    "outputs = bert_lm.cls.predictions.transform(last_hidden_state)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "emb1 = outputs[:1, 0]\n",
    "emb2 = outputs[1:, 0]\n",
    "torch.cosine_similarity(emb1, emb2)\n",
    "def l2_dist(a, b, dim=-1):\n",
    "    return ((a - b)**2).sum(dim=dim)\n",
    "\n",
    "l2_dist(emb1, emb2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([215.8967], grad_fn=<SumBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "exp_path = '../exp/0hp_unseen_path2/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "mention2id = torch.load(exp_path+'mention2id')\n",
    "pack = torch.load(exp_path+'pack.bin')\n",
    "name_array = torch.load(exp_path+'name.bin')\n",
    "ent_total = len(name_array)\n",
    "triples = torch.load(exp_path+'triples.bin')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "def get_nei(triples, max_length, ent_total):\n",
    "    from collections import defaultdict\n",
    "    import copy\n",
    "    neis = [{} for i in range(max_length+1)] # neis[i] stores i-hop neighbors\n",
    "    \n",
    "    neis[0] = {e:{e} for e in range(ent_total)}\n",
    "\n",
    "    for i in range(ent_total):\n",
    "        neis[1][i] = set()\n",
    "    for h, r, t in triples:\n",
    "        neis[1][h].add(t)\n",
    "        neis[1][t].add(h)\n",
    "    \n",
    "    for length in range(2, max_length+1):\n",
    "        nei_1 = neis[1]\n",
    "        nei_last = neis[length-1]\n",
    "        nei = neis[length]\n",
    "        for center in range(ent_total):\n",
    "            nei[center] = copy.deepcopy(nei_1[center])\n",
    "            for i in nei_1[center]:\n",
    "                nei[center] = nei[center].union(nei_last[i])\n",
    "    for i in range(5):\n",
    "        for j in range(i+1, 6):\n",
    "            for e in range(ent_total):\n",
    "                neis[-i-1][e] -= neis[-j-1][e]\n",
    "\n",
    "    return neis\n",
    "neis = get_nei(triples, 5, ent_total)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "labels = pack['labels'][:, 0]\n",
    "N = len(labels)\n",
    "\n",
    "results = []\n",
    "for depth in range(6):\n",
    "    result = 0\n",
    "    for i in range(N):\n",
    "        topk = pack['idx'][i][:1].tolist()\n",
    "        result += len(neis[depth][int(labels[i])].intersection(topk)) / 1 # top 1\n",
    "    result /= N\n",
    "    print(f'{depth}-hop neighbor in top{1} = {result}')\n",
    "    results.append(result)\n",
    "\n",
    "print('sum = ', sum(results))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0-hop neighbor in top1 = 0.6859903381642513\n",
      "1-hop neighbor in top1 = 0.04710144927536232\n",
      "2-hop neighbor in top1 = 0.05404589371980676\n",
      "3-hop neighbor in top1 = 0.02717391304347826\n",
      "4-hop neighbor in top1 = 0.023852657004830916\n",
      "5-hop neighbor in top1 = 0.021437198067632852\n",
      "sum =  0.8596014492753624\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from collections import defaultdict\n",
    "child2parent = defaultdict(set)\n",
    "for h, r, t in triples:\n",
    "    if r == 'is_a':\n",
    "        child2parent[h].add(t)\n",
    "\n",
    "E = len(name_array)\n",
    "siblings = set()\n",
    "for a in range(E):\n",
    " for c in range(a+1, E):\n",
    "  if len(child2parent[a].intersection(child2parent[c])) > 0:\n",
    "   siblings.add((a,c))\n",
    "   siblings.add((c,a))\n",
    "\n",
    "grandpas = set()\n",
    "grandsons = set()\n",
    "for aa in range(E):\n",
    " for b in child2parent[aa]:\n",
    "  for c in child2parent[b]:\n",
    "   grandpas.add((aa,c))\n",
    "   grandsons.add((c,aa))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# %%\n",
    "from tqdm import tqdm\n",
    "entity_set = {}\n",
    "for aa, c in siblings:\n",
    "    entity_set[aa] = None\n",
    "    entity_set[c] = None\n",
    "for aa, c in grandsons:\n",
    "    entity_set[aa] = None\n",
    "    entity_set[c] = None\n",
    "entity_ids = list(entity_set.keys())\n",
    "names = [name_array[i] for i in entity_ids]\n",
    "inputs = tokenizer(names, return_tensors='pt', max_length=60, padding='max_length')\n",
    "#%%\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "dataloader = DataLoader(dataset=TensorDataset(input_ids, attention_mask), batch_size=32, shuffle=False)\n",
    "name_emb = []\n",
    "\n",
    "bert_lm.eval()\n",
    "bert_lm.cuda()\n",
    "with torch.no_grad():\n",
    "    for i, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "        input_ids = input_ids.cuda()\n",
    "        attention_mask = attention_mask.cuda()\n",
    "\n",
    "        tmp = bert_lm.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
    "        tmp = bert_lm.cls.predictions.transform(tmp)\n",
    "        name_emb.append(tmp.cpu())\n",
    "name_emb = torch.cat(name_emb, dim=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "siblings_l2 = l2_dist(name_emb[[aa for (aa, c) in siblings]], name_emb[[c for (aa, c) in siblings]])\n",
    "grandpas_l2 = l2_dist(name_emb[[aa for (aa, c) in grandpas]], name_emb[[c for (aa, c) in grandpas]])\n",
    "grandsons_l2 = l2_dist(name_emb[[aa for (aa, c) in grandsons]], name_emb[[c for (aa, c) in grandsons]])\n",
    "child_parent = [(h,t) for h,r,t in triples if r == 'is_a']\n",
    "parent_l2 = l2_dist(name_emb[[aa for (aa, c) in child_parent]], name_emb[[c for (aa, c) in child_parent]])\n",
    "assert grandsons_l2.mean() == grandpas_l2.mean()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c4b208095bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchild_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtriples\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'is_a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparent_l2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maa\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild_parent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild_parent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mgrandsons_l2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgrandpas_l2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "siblings_l2.mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(43.1007)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "grandpas_l2.mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(43.4708)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "parent_l2.mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(43.1485)"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "import numpy as np\n",
    "import Levenshtein\n",
    "siblings_edit = np.mean([Levenshtein.distance(name_array[a], name_array[c]) for a,c in siblings])\n",
    "grandpas_edit = np.mean([Levenshtein.distance(name_array[a], name_array[c]) for a,c in grandpas])\n",
    "grandsons_edit = np.mean([Levenshtein.distance(name_array[a], name_array[c]) for a,c in grandsons])\n",
    "\n",
    "child_parent = [(h,t) for h,r,t in triples if r == 'is_a']\n",
    "parent_edit = np.mean([Levenshtein.distance(name_array[a], name_array[c]) for a,c in child_parent])\n",
    "assert grandsons_edit.mean() == grandpas_edit.mean()\n",
    "\n",
    "print(f'siblings_edit = {siblings_edit}')\n",
    "print(f'grandpas_edit = {grandpas_edit}')\n",
    "print(f'parent_edit = {parent_edit}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "siblings_edit = 19.415777908783966\n",
      "grandpas_edit = 24.503273590310172\n",
      "parent_edit = 18.355697810789678\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "import numpy as np\n",
    "import Levenshtein\n",
    "siblings_edit = {}\n",
    "grandpas_edit = {}\n",
    "grandsons_edit = {}\n",
    "parent_edit = {}\n",
    "child_edit = {}\n",
    "\n",
    "for a, c in siblings:\n",
    "    dist = Levenshtein.distance(name_array[a], name_array[c])\n",
    "    if dist < siblings_edit[]\n",
    "\n",
    "print(f'siblings_edit = {siblings_edit}')\n",
    "print(f'grandpas_edit = {grandpas_edit}')\n",
    "print(f'parent_edit = {parent_edit}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "import numpy as np\n",
    "import Levenshtein\n",
    "\n",
    "ret = []\n",
    "for syn, ent in mention2id.items():\n",
    "    ent = name_array[ent]\n",
    "    if syn == ent:\n",
    "        continue\n",
    "    ret.append(Levenshtein.distance(syn, ent))\n",
    "print(np.mean(ret))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16.89960139511709\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "synonym_list, dictionaries = torch.load('/tmp/synonym_list')\n",
    "datasets = set(synonym_list.keys())\n",
    "assert set(synonym_list.keys()) == set(dictionaries.keys())\n",
    "ret = {}\n",
    "for i in datasets:\n",
    "    tmp = []\n",
    "    for ent, syns in synonym_list[i].items():\n",
    "        if ent not in dictionaries[i]:\n",
    "            continue\n",
    "        ent = dictionaries[i][ent]\n",
    "        for syn in syns:\n",
    "            if syn == ent:\n",
    "                continue\n",
    "            tmp.append(Levenshtein.distance(syn, ent)) \n",
    "    ret[i] = np.mean(tmp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "ret"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bc5cdr-chemical': 19.318244170096023,\n",
       " 'bc5cdr-disease': 13.852631578947369,\n",
       " 'ncbi-disease': 17.92057761732852}"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('rwre': conda)"
  },
  "interpreter": {
   "hash": "472763105dfca07e39c0abbfe297659046d81ad244514782c862de3c564a6a12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}